---
title: ADMM Scratchpad
author: Sam Buchanan
format:
    html:
        code-fold: show
---

## Code imports

```{python}
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import treescope

treescope.basic_interactive_setup(autovisualize_arrays=True)
jax.device_count()
jax.config.update('jax_enable_x64', False)
```


```python
m = 128
n = 32

key = jax.random.key(1337)
key_w, key_g, key = jax.random.split(key, 3)
W = jax.random.normal(key_w, (m, n))
G = jax.random.normal(key_g, (m, n))
G = G / jnp.sum(G**2)**0.5
W, R = jnp.linalg.qr(W)
```

```python
# Want to solve min_X \| G + 4W @ (X + X.mT) \|_2^2


@jax.jit
def loss(X):
    return jnp.linalg.norm(G + 2 * W @ (X + X.mT), ord="nuc")
```

```python
key, sk = jax.random.split(key)
Xinit = jax.random.normal(sk, (n, n))

U, S, V = jnp.linalg.svd(G + 2 * W @ (X + X.mT))
assert jnp.allclose(loss(X), S.sum())
```

```python
# subgradient baseline
steps = 1000
losses_sgd = []
eta = 1e-2
X = Xinit
for step in range(steps):
    v, grad = jax.value_and_grad(loss)(X)
    losses_sgd.append(v)
    X = X - eta * grad
losses_sgd = jnp.array(losses_sgd)
```
```python
plt.plot(losses_sgd)
plt.show()
```


```python
# ADMM iteration
rho = 1
losses_admm = []
X = Xinit
Z = G + 2 * W @ (Xinit + Xinit.mT)
Lam = jnp.zeros((m, n))


@jax.jit
def sym(X):
    return 0.5 * (X + X.mT)


@jax.jit
def admm_step(X, Z, Lam):
    Xp = 1 / 4 * sym(W.mT @ (1 / rho * Lam + Z - G))
    U, s, VT = jnp.linalg.svd(G + 4 * W @ sym(Xp) - 1 / rho * Lam, full_matrices=False)
    sp = jnp.maximum(s - 1 / rho, 0)
    Zp = U @ jnp.diag(sp) @ VT
    Lamp = Lam + rho * (Zp - 4 * W @ sym(Xp) - G)
    return Xp, Zp, Lamp

```

```python
for step in range(steps):
    X, Z, Lam = admm_step(X, Z, Lam)
    losses_admm.append(loss(X))
losses_admm = jnp.array(losses_admm)
```


```python
plt.semilogy(losses_admm)
plt.semilogy(losses_sgd)
plt.show()
```
```python
Z.dtype
```

```python
jnp.linalg.svd(G + 4 * W @ sym(X))
```

